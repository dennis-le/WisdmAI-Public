{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "777682e0",
   "metadata": {},
   "source": [
    "# Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3ecd860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: preprocessor in /opt/conda/lib/python3.9/site-packages (1.1.3)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tweetpy (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for tweetpy\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install preprocessor\n",
    "!pip install tweetpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "186723ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import preprocessor as pre\n",
    "import regex as re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8adb12ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_twitter(): \n",
    "\n",
    "    # Set tickers and max tweets\n",
    "    tickers = ['GME','TSLA','TWTR','AMC','SPY','HMHC','DWAC','AMD','SST','AAPL','AMZN','NVDA','TLRY','NFLX','QQQ','PLTR','FB','BABA','VIX','SOFI','TEAM','RBLX','RSX','WISH','OSU']\n",
    "    max_tweets = 100 #max per ticker\n",
    "\n",
    "    # Set date/time parameters\n",
    "    cur_time_utc = datetime.utcnow().replace(microsecond=0)\n",
    "    until_date = cur_time_utc.strftime(\"%Y-%m-%d\") #\"2022-04-03\"#\"2022-04-25\"#\n",
    "    from_date =  cur_time_utc - timedelta(days=1) #\"2022-04-02\"#\"2022-04-24\"#\n",
    "    from_date = from_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Authentication\n",
    "    ##TO DO ##\n",
    "    consumer_key = \"todo\"\n",
    "    consumer_secret = \"todo\"\n",
    "\n",
    "    \n",
    "    auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "    # Create a wrapper for the Twitter API\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "    # Function: preprocess tweet text\n",
    "    def tweetprocess(tweet):\n",
    "        #https://towardsdatascience.com/basic-tweet-preprocessing-in-python-efd8360d529e\n",
    "        cleantweet = pre.clean(tweet)\n",
    "        cleantweet = cleantweet.lower()\n",
    "        cleantweet = re.sub(\"\\d+\", \"\", cleantweet)\n",
    "        cleantweet = re.sub(r'[^\\w\\s]', '', cleantweet)   \n",
    "        return cleantweet\n",
    "\n",
    "    # Function: identify tweet quality\n",
    "    def tweetquality(user_verified, favorite_count, retweet_count):\n",
    "        if user_verified == True or favorite_count > 100 or retweet_count > 10:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # Function: twitter search pagination and rate limit handling\n",
    "    def limit_handled(cursor):\n",
    "      while True:\n",
    "          try:\n",
    "              yield cursor.next()\n",
    "          except StopIteration:\n",
    "              break\n",
    "          except tweepy.error:\n",
    "              print('Reached rate limit. Sleeping for >15 minutes')\n",
    "              time.sleep(15 * 61)\n",
    "\n",
    "    # Function: obtain query list from ticker list\n",
    "    def getqueries(tickers): #returns a list of query strings\n",
    "        queries = []\n",
    "        for ticker in tickers:\n",
    "            querylist = []\n",
    "            querylist.append('$'+ticker)\n",
    "            othertickers=[]\n",
    "            [othertickers.append(t) for t in tickers]\n",
    "            othertickers.remove(ticker)\n",
    "            for otherticker in othertickers:\n",
    "                querylist.append(' -$'+otherticker) \n",
    "            tickerquery = ''.join([str(q) for q in querylist]) + ' -filter:retweets' #Exclude retweets\n",
    "            queries.append(tickerquery)\n",
    "        return queries\n",
    "\n",
    "    # Function: get tweets using tweepy\n",
    "    def get_tweets(query, since_id, until_date, max_tweets):\n",
    "\n",
    "        #search for tweets using Tweepy\n",
    "        search = limit_handled(tweepy.Cursor(api.search #https://docs.tweepy.org/en/stable/api.html#search-tweets\n",
    "                            ,q=query\n",
    "                            ,tweet_mode='extended'\n",
    "                            ,lang='en'\n",
    "                            ,result_type=\"recent\"\n",
    "                            ,since_id = since_id\n",
    "                            ,until=until_date\n",
    "                            ).items(max_tweets))\n",
    "        dftweets = pd.DataFrame()\n",
    "        for tweet in search:\n",
    "            dftweets = dftweets.append(pd.json_normalize(tweet._json))\n",
    "        print(query,'\\n','# tweets:',len(dftweets))\n",
    "\n",
    "        # calculate attributes\n",
    "        try:\n",
    "            dftweets['full_text_preprocessed'] = dftweets.apply(lambda row : tweetprocess(row['full_text']), axis = 1)\n",
    "            dftweets['quality'] = dftweets.apply(lambda row : tweetquality(row['user.verified'], row['favorite_count'], row['retweet_count']), axis = 1)\n",
    "            dftweets['num_cashtags'] = dftweets.apply(lambda row : str(row['entities.symbols']).count('text'), axis = 1)\n",
    "            dftweets['ticker'] = dftweets.apply(lambda row : query.split()[0], axis = 1)\n",
    "            dftweets['query_params'] = dftweets.apply(lambda row : 'query:'+query+' since_id:'+str(since_id)+' until_date:'+str(until_date)+' max_tweets:'+str(max_tweets), axis = 1)\n",
    "            #apply filter\n",
    "            dftweets = dftweets[dftweets.num_cashtags == 1]\n",
    "            # return output\n",
    "            return dftweets[['id','ticker','created_at','full_text_preprocessed','user.verified','favorite_count','retweet_count','quality','entities.symbols','num_cashtags','query_params']]\n",
    "        except Exception:\n",
    "            pass\n",
    "        finally:\n",
    "            print(' # tweets (filtered):',len(dftweets),'\\n')\n",
    "\n",
    "      \n",
    "\n",
    "    # Find the last tweet id for from_date (need this to filter on from_date)\n",
    "    search_since_id = limit_handled(tweepy.Cursor(api.search #https://docs.tweepy.org/en/stable/api.html#search-tweets\n",
    "                            ,q='A'\n",
    "                            ,tweet_mode='extended'\n",
    "                            ,lang='en'\n",
    "                            ,result_type=\"recent\"\n",
    "                            ,until=from_date\n",
    "                            ).items(1))\n",
    "    since_id  = [tweet._json['id'] for tweet in search_since_id][0]\n",
    "\n",
    "    #Get the tweets\n",
    "    queries = getqueries(tickers)\n",
    "    dftweets_tofile = pd.DataFrame()\n",
    "    for query in queries:\n",
    "        dftweets_tofile = dftweets_tofile.append(get_tweets(query, since_id, until_date, max_tweets))\n",
    "\n",
    "    return dftweets_tofile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d473ec2f",
   "metadata": {},
   "source": [
    "# Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "942affb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in /opt/conda/lib/python3.9/site-packages (7.6.0)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in /opt/conda/lib/python3.9/site-packages (from praw) (2.3.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /opt/conda/lib/python3.9/site-packages (from praw) (1.3.3)\n",
      "Requirement already satisfied: update-checker>=0.18 in /opt/conda/lib/python3.9/site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /opt/conda/lib/python3.9/site-packages (from prawcore<3,>=2.1->praw) (2.26.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /opt/conda/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.25.7)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /opt/conda/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install praw\n",
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e8344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TO DO \n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"TODO\",\n",
    "    client_secret=\"TODO\",\n",
    "    password=\"TODO\",\n",
    "    user_agent=\"TODO\",\n",
    "    username=\"TODO\",\n",
    "    check_for_async=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40919a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from praw.models import MoreComments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2f58dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns df of reddit posts for model\n",
    "def pull_reddit(): \n",
    "  #stocklist = ('gme', 'amc', 'tsla', 'hood', 'fb', 'twtr', 'spy', 'webr', 'evtl', 'arqq', 'bbby', 'tsm', 'ppi', 'nvda')\n",
    "  stocklist = ('nflx','tsla', 'gme','twtr','spy', 'fb','amc', 'dis','nvda', 'amd', 'amzn', 'snap', 'vix', 'qqq', 'appl', 'tlry', 'hmhc')\n",
    "  subreddits = ('wallstreetbets','StockMarket','pennystocks','GME','CryptoCurrency','stocks','investing','Superstonk')\n",
    "  posts = pd.DataFrame()\n",
    "  for stock in stocklist:\n",
    "    for sub in subreddits:\n",
    "      subreddit = reddit.subreddit(sub)\n",
    "      for post in subreddit.search(stock, sort = 'new', time_filter = \"week\",limit=None):\n",
    "        if stock not in post.title.lower():\n",
    "          continue\n",
    "        pslice = pd.DataFrame()\n",
    "        pslice = pslice.append({\n",
    "            'stock': stock,\n",
    "            'subreddit': post.subreddit,\n",
    "            'type': 'POST',\n",
    "            'post_title': post.title,\n",
    "            'selftext': post.selftext,\n",
    "            #'upvote_ratio': post.upvote_ratio,\n",
    "            #'ups': post.ups,\n",
    "            'ups': post.score,\n",
    "            #'num_comments': post.num_comments,\n",
    "            'id': post.id,\n",
    "            'time': datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "          }, ignore_index=True)\n",
    "        #if post.id in posts['id']:\n",
    "          #continue\n",
    "        posts = posts.append(pslice, ignore_index=True)\n",
    "        #post.comments.replace_more(limit=None)\n",
    "        cslice = pd.DataFrame()\n",
    "        for com in post.comments:\n",
    "          if isinstance(com, MoreComments):\n",
    "            continue\n",
    "          cslice = cslice.append({\n",
    "              'stock': stock,\n",
    "              'subreddit': post.subreddit,\n",
    "              'type': 'COMMENT',\n",
    "              'post_title': post.title,\n",
    "              'selftext': com.body,\n",
    "              #'upvote_ratio': com.upvote_ratio,\n",
    "              #'ups': com.ups,\n",
    "              'ups': com.score,\n",
    "              #'num_comments': post.num_comments,\n",
    "              'id': post.id,\n",
    "              'time': datetime.fromtimestamp(com.created_utc).strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            }, ignore_index=True)\n",
    "        posts = posts.append(cslice, ignore_index=True)\n",
    "\n",
    "  return posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c05006",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b740f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests \n",
    "from time import sleep\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4886b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$GME -$TSLA -$TWTR -$AMC -$SPY -$HMHC -$DWAC -$AMD -$SST -$AAPL -$AMZN -$NVDA -$TLRY -$NFLX -$QQQ -$PLTR -$FB -$BABA -$VIX -$SOFI -$TEAM -$RBLX -$RSX -$WISH -$OSU -filter:retweets \n",
      " # tweets: 618\n",
      " # tweets (filtered): 618 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitter = pull_twitter()\n",
    "reddit = pull_reddit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f599e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a4b3a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment function both reddit and twitter \n",
    "\n",
    "headers = {\"TODO\" }\n",
    "\n",
    "# Model 1: https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest\n",
    "\n",
    "model1 = \"https://api-inference.huggingface.co/models/cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "def get_sentiment(string, model, type = None):\n",
    "    #string - text to run through model \n",
    "    #model - model url (reference above) \n",
    "    #output types: score, label \n",
    "    done = False\n",
    "    \n",
    "    headers = {\"TO DO\"}\n",
    "    while not done:\n",
    "        try: \n",
    "            #access model + obtain ouput\n",
    "            payload = query = {\"inputs\": string}\n",
    "            print(payload)\n",
    "            response = requests.post(model, headers = headers, json = query) \n",
    "            print(response.json())\n",
    "            output = response.json()[0]\n",
    "            #print(output)\n",
    "\n",
    "            best = max(output, key = lambda x: x['score'])\n",
    "            label = best['label'].lower()\n",
    "            score = np.round(best['score'], decimals = 3)\n",
    "            done = True \n",
    "        except Exception as KeyError: \n",
    "            pass\n",
    "            if KeyError:\n",
    "                sleep(20)  \n",
    "    \n",
    "    #desired output\n",
    "    if type == \"score\": \n",
    "        return score\n",
    "    if type == \"label\": \n",
    "        return label\n",
    "\n",
    "    return label, score\n",
    "    \n",
    "\n",
    "vec_sentiment = np.vectorize(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "766de747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reddit\n",
    "#gets sample post and comments \n",
    "def get_text(post_titles, comments): \n",
    "    \n",
    "    if (len(post_titles) + len(comments)) < 150: \n",
    "        sample_titles, sample_comments = post_titles, comments\n",
    "        text = np.append(sample_titles, sample_comments)\n",
    "        return text \n",
    "\n",
    "    if (len(post_titles) <= 30):    \n",
    "        sample_titles = post_titles\n",
    "\n",
    "    if len(post_titles) > 30: \n",
    "        sample_titles = np.random.choice(post_titles, 30)\n",
    "\n",
    "    size = 150 - len(sample_titles)\n",
    "\n",
    "    if size >= len(comments):\n",
    "        sample_comments = comments \n",
    "\n",
    "    elif size < len(comments): \n",
    "        sample_comments = np.random.choice(comments, size) \n",
    "\n",
    "    text = np.append(sample_titles, sample_comments)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7583a925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns dictionary of dataframes for each stock ticker\n",
    "\n",
    "#returns dictionary of dataframes for each stock ticker\n",
    "def split_by_ticker(data, column):\n",
    "    if column == \"Reddit\": \n",
    "       text = 'stock'\n",
    "    if column == \"Twitter\": \n",
    "       text = 'ticker' \n",
    "\n",
    "    stock_tickers = data[text].unique() \n",
    "    DataFrameDict = {elem : pd.DataFrame for elem in stock_tickers}\n",
    "  \n",
    "    for key in DataFrameDict.keys():\n",
    "        if column == \"Twitter\":\n",
    "          columns = {'full_text_preprocessed': 'text'}\n",
    "          DataFrameDict[key] = data[data[text] == key].copy().rename(columns = columns)\n",
    "        else: \n",
    "          DataFrameDict[key] = data[data[text] == key].copy()\n",
    "\n",
    "    return DataFrameDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b27bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_Reddit(ticker_df): \n",
    "\n",
    "    models = [model1] \n",
    "    model_dict = {model1: 'model1'}\n",
    "    \n",
    "    post_titles = ticker_df[ticker_df['type'] == 'POST']['post_title'].unique()\n",
    "    long_comments = ticker_df[(ticker_df['type'] == 'COMMENT') & (ticker_df['selftext'] != '[deleted]')].dropna(subset = ['selftext'])\n",
    "    comments = long_comments[long_comments['selftext'].str.len() < 1000]['selftext'].values\n",
    "    \n",
    "    mentions = (len(post_titles) + len(long_comments))\n",
    "    text = get_text(post_titles, comments)\n",
    "\n",
    "    data = pd.DataFrame({'text': text}).reset_index()\n",
    "\n",
    "    label = model_dict[model1] + \"Sentiment\" \n",
    "    result_label = model_dict[model1] + \"Score\"  \n",
    "\n",
    "    sentiment, score = vec_sentiment(text, model1)\n",
    "    data[label] = sentiment \n",
    "    data[result_label] = score \n",
    "\n",
    "    return mentions, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "550d2271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run sentiment model on dataframe for one stock ticker\n",
    "#returns mentions and tuple -  (sentiment, score)\n",
    "def run_Twitter(ticker_df):   \n",
    "    models = [model1] \n",
    "    model_dict = {model1: 'model1'}\n",
    "    mentions = len(ticker_df) \n",
    "\n",
    "    ticker_df_filt = ticker_df[(ticker_df['favorite_count'] >= 2) | (ticker_df['retweet_count'] >= 1)].copy()\n",
    "    \n",
    "    if len(ticker_df_filt) > 150:\n",
    "        data = ticker_df_filt.sample(150)\n",
    "\n",
    "    elif len(ticker_df_filt) < 30: \n",
    "        if len(ticker_df) > 150:\n",
    "            data = ticker_df.sample(150) \n",
    "        else: \n",
    "            data = ticker_df\n",
    "\n",
    "    else:\n",
    "        data = ticker_df_filt.copy()\n",
    "\n",
    "    text = data['text'].values \n",
    "     \n",
    "    label = model_dict[model1] + \"Sentiment\" \n",
    "    result_label = model_dict[model1] + \"Score\"  \n",
    "\n",
    "    sentiment, score = vec_sentiment(text, model1)\n",
    "    data[label] = sentiment \n",
    "    data[result_label] = score \n",
    "\n",
    "    return mentions, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d5e4efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns sentiment proportions + determines majority sentinment based \n",
    "def process_Sentiment(sentimment_Data): \n",
    "    neg_avg = sentimment_Data[sentimment_Data['model1Sentiment'] == 'negative']['model1Score'].values \n",
    "   \n",
    "    pos_avg = sentimment_Data[sentimment_Data['model1Sentiment'] == 'positive']['model1Score'].values \n",
    "\n",
    "    neu_avg = sentimment_Data[sentimment_Data['model1Sentiment'] == 'neutral']['model1Score'].values\n",
    "    \n",
    "    total_vals = len(sentimment_Data)\n",
    "\n",
    "    positive_per = np.round((len(pos_arr)/total_vals) * 100, 2)\n",
    "    negative_per = np.round((len(neg_arr)/total_vals) * 100, 2)\n",
    "    neutral_per =  np.round((len(neu_arr)/total_vals) * 100, 2)\n",
    "\n",
    "    num_neg, num_pos = len(neg_arr), len(pos_arr)\n",
    "    neg_weight, pos_weight = num_neg / total_vals, num_pos / total_vals\n",
    "    weighted_neg, weighted_pos = neg_avg * neg_weight, pos_avg * pos_weight \n",
    "\n",
    "    try:\n",
    "        if (num_neg / num_pos) > 0.75 and (num_neg / num_pos) < 1.25:\n",
    "            sentimment_ratio = neg_avg / pos_avg\n",
    "        else: \n",
    "            sentimment_ratio = weighted_neg / weighted_pos\n",
    "            \n",
    "    except ZeroDivisionError:\n",
    "        sentimment_ratio = 2\n",
    "    \n",
    "    final_sentimment = 'Negative'\n",
    "    if sentimment_ratio == 1:\n",
    "        final_sentimment = np.random.choice('Negative', 'Positive')\n",
    "    elif sentimment_ratio < 1:\n",
    "        final_sentimment = 'Positive'\n",
    "    \n",
    "    output_dict = {'Negative Percent' : negative_per,\n",
    "                   'Positive Percent' : positive_per,\n",
    "                   'Neutral Percent': neutral_per,\n",
    "                   'Overall Sentiment' : final_sentimment}\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c103da57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# social - Reddit, Twitter \n",
    "def Model(data, social): \n",
    "    #get data dic\n",
    "    dataDic = split_by_ticker(data, social)\n",
    "    # get tickers \n",
    "\n",
    "    tickers = list(dataDic.keys())\n",
    "    mentions, negative_per, positive_per, neutral_per, overall = [], [], [], [], []\n",
    "\n",
    "    if social == \"Reddit\":\n",
    "       run_model = run_Reddit \n",
    "    if social == \"Twitter\": \n",
    "       run_model = run_Twitter\n",
    "\n",
    "    for ticker in tickers: \n",
    "        mention_count, stock = run_model(dataDic[ticker]) \n",
    "        results = process_Sentiment(stock)\n",
    "        \n",
    "        mentions.append(mention_count)\n",
    "        negative_per.append(results['Negative Percent'])\n",
    "        positive_per.append(results['Positive Percent'])\n",
    "        neutral_per.append(results['Neutral Percent'])\n",
    "        overall.append(results['Overall Sentiment']) \n",
    "\n",
    "\n",
    "    \n",
    "    output = pd.DataFrame({\"Ticker\": tickers,\n",
    "                          \"Mentions\": mentions, \n",
    "                          \"Negative Percent\": negative_per, \n",
    "                          \"Positive Percent\": positive_per, \n",
    "                          \"Neutral Percent\": neutral_per, \n",
    "                          \"Overall Sentiment\": overall\n",
    "                            })\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa21d41c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "f6137767cd5a1185f0adfe5ac38846a9103bfc8540bec40d423695759fa94714"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
